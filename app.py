from fastapi import FastAPI, Request, Body, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates

import joblib
import time
import numpy as np
import pandas as pd
from pathlib import Path
import json
from datetime import datetime
import asyncio

# ======================
# Setup FastAPI
# ======================
app = FastAPI()

BASE_DIR = Path(__file__).resolve().parent
TEMPLATES_DIR = BASE_DIR / "templates"
STATIC_DIR = BASE_DIR / "static"
MODELS_DIR = BASE_DIR / "models"
DATA_DIR = BASE_DIR / "data"

DATA_DIR.mkdir(exist_ok=True)

app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")
templates = Jinja2Templates(directory=TEMPLATES_DIR)

# ======================
# Load Baseline Models (A/B and more)
# ======================

MODELS_REGRESS_DIR = BASE_DIR / "models_regress"
MODELS_LINEAR_DIR  = BASE_DIR / "models_linear"
MODELS_RF_DIR      = BASE_DIR / "models_tree"   # ‚ö†Ô∏è ‡πÉ‡∏ä‡πâ models_tree ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏ó‡∏£‡∏ô
MODELS_NB_DIR      = BASE_DIR / "models_nb"
MODELS_LGBM_DIR    = BASE_DIR / "models_lgbm"
MODELS_ET_DIR      = BASE_DIR / "models_et"

# ===== Model A: Logistic Regression (default) =====
VECTORIZER_A_PATH = MODELS_REGRESS_DIR / "vectorizer_20260210_173038_59628ab2.joblib"
MODEL_A_PATH      = MODELS_REGRESS_DIR / "sentiment_model_20260210_173038_59628ab2.joblib"

for p in [VECTORIZER_A_PATH, MODEL_A_PATH]:
    if not p.exists():
        raise FileNotFoundError(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå Baseline A: {p}")

vectorizer_a = joblib.load(VECTORIZER_A_PATH)
classifier_a = joblib.load(MODEL_A_PATH)

# ===== Helper: Load model + vectorizer pair =====
def load_model_pair(vec_path, model_path, name):
    if vec_path.exists() and model_path.exists():
        vec = joblib.load(vec_path)
        model = joblib.load(model_path)
        return vec, model, True
    else:
        print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•: {name}")
        return None, None, False

# Define all models ‚Äî ‚úÖ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö timestamp 2026-02-10
model_configs = {
    "linear": {
        "vec": MODELS_LINEAR_DIR / "vectorizer_20260210_173236_e42de6e6.joblib",
        "model": MODELS_LINEAR_DIR / "sentiment_model_20260210_173236_e42de6e6.joblib",
        "name": "Linear SVM",
        "version": "TF-IDF + Linear SVM (Max-Margin)"
    },
    "rf": {
        "vec": MODELS_RF_DIR / "vectorizer_20260210_173545_2c0769a7.joblib",
        "model": MODELS_RF_DIR / "sentiment_model_20260210_173545_2c0769a7.joblib",
        "name": "Random Forest",
        "version": "TF-IDF + Random Forest"
    },
    "nb": {
        "vec": MODELS_NB_DIR / "vectorizer_20260210_173444_1c96eeb5.joblib",
        "model": MODELS_NB_DIR / "sentiment_model_20260210_173444_1c96eeb5.joblib",
        "name": "Naive Bayes",
        "version": "TF-IDF + Multinomial Naive Bayes"
    },
    "lgbm": {
        "vec": MODELS_LGBM_DIR / "vectorizer_20260210_173417_6ae59428.joblib",
        "model": MODELS_LGBM_DIR / "sentiment_model_20260210_173417_6ae59428.joblib",
        "name": "LightGBM",
        "version": "TF-IDF + LightGBM"
    },
    "et": {
        "vec": MODELS_ET_DIR / "vectorizer_20260210_173329_02c8dcc4.joblib",
        "model": MODELS_ET_DIR / "sentiment_model_20260210_173329_02c8dcc4.joblib",
        "name": "Extra Trees",
        "version": "TF-IDF + Extra Trees Classifier"
    }
}

# Load all optional models
loaded_models = {}
for key, cfg in model_configs.items():
    vec, model, ok = load_model_pair(cfg["vec"], cfg["model"], cfg["name"])
    if ok:
        loaded_models[key] = {
            "vectorizer": vec,
            "classifier": model,
            "name": cfg["name"],
            "version": cfg["version"]
        }

# ======================
# Global word sentiment (Model A only)
# ======================
def get_global_word_sentiment():
    coefs = classifier_a.coef_
    classes = classifier_a.classes_
    feature_names = vectorizer_a.get_feature_names_out()

    idx_to_sentiment = {}
    for i, cls in enumerate(classes):
        c = str(cls).lower()
        if c in ["positive", "pos", "‡∏î‡∏µ"]:
            idx_to_sentiment[i] = "positive"
        elif c in ["negative", "neg", "‡πÅ‡∏¢‡πà", "‡∏´‡πà‡∏ß‡∏¢"]:
            idx_to_sentiment[i] = "negative"
        else:
            idx_to_sentiment[i] = "neutral"

    word_sentiment = {}
    for i, word in enumerate(feature_names):
        class_idx = int(np.argmax(coefs[:, i]))
        word_sentiment[word] = idx_to_sentiment.get(class_idx, "neutral")

    return word_sentiment

GLOBAL_WORD_SENTIMENT = get_global_word_sentiment()

# ======================
# Label mapping helper
# ======================
LABEL_MAP = {
    0: "NEGATIVE",
    1: "NEUTRAL",
    2: "POSITIVE",
    "0": "NEGATIVE",
    "1": "NEUTRAL",
    "2": "POSITIVE",
}

def normalize_label(label):
    """‡πÅ‡∏õ‡∏•‡∏á label ‡πÉ‡∏î‡πÜ ‚Üí 'NEGATIVE' / 'NEUTRAL' / 'POSITIVE'"""
    if isinstance(label, str):
        label_lower = label.lower()
        if label_lower in ["negative", "neg", "‡πÅ‡∏¢‡πà", "‡∏´‡πà‡∏ß‡∏¢", "‡∏•‡∏ö", "0"]:
            return "NEGATIVE"
        elif label_lower in ["neutral", "neu", "‡∏Å‡∏•‡∏≤‡∏á", "‡πÄ‡∏â‡∏¢‡πÜ", "‡∏Å‡∏•‡∏≤‡∏á‡πÜ", "1"]:
            return "NEUTRAL"
        elif label_lower in ["positive", "pos", "‡∏î‡∏µ", "‡∏ö‡∏ß‡∏Å", "‡∏ä‡∏≠‡∏ö", "2"]:
            return "POSITIVE"
        else:
            return LABEL_MAP.get(label, label.upper())
    else:
        return LABEL_MAP.get(label, str(label).upper())

# ======================
# Helper: Important words (Baseline models)
# ======================
def get_important_words(text: str, vectorizer, classifier, top_k: int = 5):
    X = vectorizer.transform([text])
    feature_names = vectorizer.get_feature_names_out()

    # üå≥ Tree-based models: RF, LightGBM, Extra Trees
    if hasattr(classifier, "feature_importances_"):
        imp = classifier.feature_importances_
        present = X.toarray()[0]
        contributions = present * imp
        top_indices = np.argsort(contributions)[::-1][:top_k]
        words = [feature_names[i] for i in top_indices if contributions[i] > 0]
        sents = [GLOBAL_WORD_SENTIMENT.get(w, "neutral") for w in words]
        return words[:top_k], sents[:top_k]

    # üìà Linear models: LogisticRegression, LinearSVC, Naive Bayes
    elif hasattr(classifier, "coef_"):
        if hasattr(classifier, "predict_proba"):
            probs = classifier.predict_proba(X)[0]
            class_idx = int(np.argmax(probs))
            if classifier.coef_.shape[0] > 1:
                coef = classifier.coef_[class_idx]
            else:
                coef = classifier.coef_[0]
        else:
            if classifier.coef_.shape[0] == 1:
                coef = classifier.coef_[0]
            else:
                decision = classifier.decision_function(X)
                class_idx = int(np.argmax(decision))
                coef = classifier.coef_[class_idx]

        contributions = X.toarray()[0] * coef
        top_indices = np.argsort(np.abs(contributions))[::-1][:top_k]
        words, sentiments = [], []
        for i in top_indices:
            if contributions[i] == 0:
                continue
            word = feature_names[i]
            words.append(word)
            sentiments.append(GLOBAL_WORD_SENTIMENT.get(word, "neutral"))
        return words, sentiments

    # ‚ùì Fallback
    else:
        return [], []

# ======================
# Routes
# ======================

@app.get("/", response_class=HTMLResponse)
def home(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/health")
def health():
    return {
        "status": "ok",
        "baseline_a": True,
        "available_models": list(loaded_models.keys()),  # ‡πÑ‡∏°‡πà‡∏°‡∏µ bert
    }

@app.get("/model/info")
def model_info():
    info = {
        "model_a": {
            "name": "sentiment_lr",
            "version": "TF-IDF + Logistic Regression (Linear, Probabilistic)",
            "file": MODEL_A_PATH.name,
        }
    }
    for key, mdl in loaded_models.items():
        info[key] = {
            "name": mdl["name"],
            "version": mdl["version"],
        }
    return info

@app.get("/errors", response_class=HTMLResponse)
def show_errors(request: Request):
    all_errors = []
    seen = set()

    try:
        errors_path = DATA_DIR / "error_examples.csv"
        if errors_path.exists():
            df_static = pd.read_csv(errors_path)
            for _, row in df_static.iterrows():
                text = str(row.get("text", "")).strip()
                true_label = str(row.get("true_label", "?"))
                pred_label = str(row.get("pred_label", "?"))
                key = f"{text}|{true_label}|{pred_label}"
                if key not in seen:
                    seen.add(key)
                    all_errors.append({
                        "text": text,
                        "true_label": true_label,
                        "pred_label": pred_label,
                        "source": "train_misclassified",
                    })
    except Exception as e:
        print(f"Error loading static errors: {e}")

    try:
        feedback_path = DATA_DIR / "feedback_log.jsonl"
        if feedback_path.exists():
            with open(feedback_path, "r", encoding="utf-8") as f:
                lines = f.readlines()
            for line in reversed(lines):
                if line.strip():
                    try:
                        fb = json.loads(line)
                        if fb.get("feedback") == "incorrect":
                            text = str(fb.get("text", "")).strip()
                            model = fb.get("model", "")
                            key = f"{text}|{model}"
                            if key not in seen:
                                seen.add(key)
                                all_errors.append({
                                    "text": text,
                                    "true_label": str(fb.get("true_label", "UNKNOWN")),
                                    "pred_label": str(fb.get("predicted_label", "?")),
                                    "source": "user_feedback",
                                    "model": model,
                                    "timestamp": fb.get("timestamp", ""),
                                })
                    except Exception as ex:
                        print(f"Invalid feedback line: {ex}")
                        continue
    except Exception as e:
        print(f"Error loading feedback: {e}")

    all_errors.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
    errors_to_show = all_errors[:20]

    return templates.TemplateResponse(
        "errors.html", {"request": request, "errors": errors_to_show}
    )

# ======================
# Predict Endpoints
# ======================

@app.post("/predict")
def predict(text: str = Body(..., embed=True)):
    start = time.time()
    X = vectorizer_a.transform([text])
    pred_raw = classifier_a.predict(X)[0]
    pred = normalize_label(pred_raw)
    prob = float(np.max(classifier_a.predict_proba(X)[0]))
    latency = (time.time() - start) * 1000
    words, sents = get_important_words(text, vectorizer_a, classifier_a)

    return {
        "label": pred,
        "confidence": round(prob, 2),
        "latency_ms": round(latency, 2),
        "model": "sentiment_lr",
        "version": "TF-IDF + Logistic Regression (Linear, Probabilistic)",
        "important_words": words,
        "word_sentiments": sents,
    }

@app.post("/predict-ab")
def predict_ab(
    text: str = Body(..., embed=True),
    model_b_type: str = Body("linear", embed=True)
):
    # ===== Model A =====
    start_a = time.time()
    Xa = vectorizer_a.transform([text])
    pred_a_raw = classifier_a.predict(Xa)[0]
    pred_a = normalize_label(pred_a_raw)
    prob_a = float(np.max(classifier_a.predict_proba(Xa)[0]))
    latency_a = (time.time() - start_a) * 1000
    words_a, sents_a = get_important_words(text, vectorizer_a, classifier_a)

    result = {
        "model_a": {
            "label": pred_a,
            "confidence": round(prob_a, 2),
            "latency_ms": round(latency_a, 2),
            "model_name": "sentiment_lr",
            "version": "TF-IDF + Logistic Regression",
            "important_words": words_a,
            "word_sentiments": sents_a,
        },
        "model_b": None
    }

    # ===== Model B =====
    if model_b_type in loaded_models:
        mdl = loaded_models[model_b_type]
        start_b = time.time()
        Xb = mdl["vectorizer"].transform([text])
        pred_b_raw = mdl["classifier"].predict(Xb)[0]
        pred_b = normalize_label(pred_b_raw)

        # Confidence logic
        if hasattr(mdl["classifier"], "predict_proba"):
            prob_b = float(np.max(mdl["classifier"].predict_proba(Xb)[0]))
        elif hasattr(mdl["classifier"], "decision_function"):
            score = mdl["classifier"].decision_function(Xb)
            prob_b = float(1 / (1 + np.exp(-abs(score.ravel()[0]))))
        else:
            prob_b = 0.95  # fallback

        latency_b = (time.time() - start_b) * 1000
        words_b, sents_b = get_important_words(text, mdl["vectorizer"], mdl["classifier"])

        result["model_b"] = {
            "label": pred_b,
            "confidence": round(prob_b, 2),
            "latency_ms": round(latency_b, 2),
            "model_name": mdl["name"],
            "version": mdl["version"],
            "important_words": words_b,
            "word_sentiments": sents_b,
        }

    else:
        available = list(loaded_models.keys())
        raise HTTPException(
            status_code=400,
            detail=f"Model B type '{model_b_type}' not available. Available: {available}"
        )

    return result

# ======================
# Feedback Logging
# ======================
FEEDBACK_LOG_PATH = DATA_DIR / "feedback_log.jsonl"

@app.post("/feedback")
async def log_feedback(request: Request):
    try:
        data = await request.json()
        required_fields = ["text", "model", "predicted_label", "feedback"]
        if not all(field in data for field in required_fields):
            return JSONResponse({"error": "Missing required fields"}, status_code=400)

        if "timestamp" not in data:
            data["timestamp"] = datetime.utcnow().isoformat()

        with open(FEEDBACK_LOG_PATH, "a", encoding="utf-8") as f:
            f.write(json.dumps(data, ensure_ascii=False) + "\n")

        return {"status": "success", "message": "Feedback recorded"}
    except Exception as e:
        print(f"Feedback error: {e}")
        return JSONResponse({"error": "Failed to log feedback"}, status_code=500)

# ======================
# Background Task
# ======================
async def rotate_feedback_periodically(max_lines=100):
    while True:
        await asyncio.sleep(600)
        try:
            if FEEDBACK_LOG_PATH.exists():
                with open(FEEDBACK_LOG_PATH, "r", encoding="utf-8") as f:
                    lines = f.readlines()
                if len(lines) > max_lines:
                    recent_lines = lines[-max_lines:]
                    with open(FEEDBACK_LOG_PATH, "w", encoding="utf-8") as f:
                        f.writelines(recent_lines)
                    print(f"[{datetime.now()}] üîÑ ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î feedback ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {max_lines} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        except Exception as e:
            print(f"[{datetime.now()}] ‚ùå rotate feedback error: {e}")

@app.on_event("startup")
async def start_background_tasks():
    asyncio.create_task(rotate_feedback_periodically())